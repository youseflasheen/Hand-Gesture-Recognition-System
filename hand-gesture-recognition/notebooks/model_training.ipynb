{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training for Hand Gesture Recognition\n",
    "\n",
    "This notebook is used for training the model to recognize five American Sign Language (ASL) signs. It includes steps for loading the dataset, training the model, and evaluating its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_1780\\1535413110.py:100: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 127ms/step - accuracy: 0.9557 - loss: 0.1269 - val_accuracy: 1.0000 - val_loss: 4.4562e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 114ms/step - accuracy: 0.9994 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 4.5141e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 112ms/step - accuracy: 0.9991 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 7.6488e-05\n",
      "Epoch 4/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 115ms/step - accuracy: 0.9993 - loss: 0.0022 - val_accuracy: 1.0000 - val_loss: 5.7710e-06\n",
      "Epoch 5/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 114ms/step - accuracy: 0.9994 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 1.8143e-05\n",
      "Epoch 6/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 116ms/step - accuracy: 1.0000 - loss: 3.9804e-04 - val_accuracy: 1.0000 - val_loss: 4.6614e-05\n",
      "Epoch 7/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 116ms/step - accuracy: 0.9998 - loss: 8.1221e-04 - val_accuracy: 1.0000 - val_loss: 9.2756e-05\n",
      "Epoch 8/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 116ms/step - accuracy: 1.0000 - loss: 1.6138e-04 - val_accuracy: 1.0000 - val_loss: 1.2669e-05\n",
      "Epoch 9/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 118ms/step - accuracy: 0.9996 - loss: 8.0970e-04 - val_accuracy: 1.0000 - val_loss: 1.8255e-06\n",
      "Epoch 10/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 116ms/step - accuracy: 0.9995 - loss: 0.0011 - val_accuracy: 1.0000 - val_loss: 2.0174e-05\n",
      "Epoch 1/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 171ms/step - accuracy: 0.9813 - loss: 0.0889 - val_accuracy: 0.9998 - val_loss: 2.8954e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 127ms/step - accuracy: 0.9989 - loss: 0.0061 - val_accuracy: 1.0000 - val_loss: 2.9850e-09\n",
      "Epoch 3/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 125ms/step - accuracy: 0.9991 - loss: 0.0054 - val_accuracy: 0.9998 - val_loss: 2.5625e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 130ms/step - accuracy: 0.9998 - loss: 0.0027 - val_accuracy: 0.9998 - val_loss: 1.4906e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 130ms/step - accuracy: 0.9997 - loss: 6.1800e-04 - val_accuracy: 1.0000 - val_loss: 1.6310e-08\n",
      "Epoch 6/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 129ms/step - accuracy: 0.9996 - loss: 0.0020 - val_accuracy: 0.9998 - val_loss: 9.4146e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 131ms/step - accuracy: 0.9997 - loss: 0.0018 - val_accuracy: 1.0000 - val_loss: 2.8656e-10\n",
      "Epoch 8/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 133ms/step - accuracy: 0.9997 - loss: 3.2795e-04 - val_accuracy: 1.0000 - val_loss: 4.0596e-10\n",
      "Epoch 9/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 139ms/step - accuracy: 0.9999 - loss: 2.9045e-04 - val_accuracy: 1.0000 - val_loss: 9.5307e-07\n",
      "Epoch 10/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 135ms/step - accuracy: 0.9999 - loss: 2.7632e-04 - val_accuracy: 1.0000 - val_loss: 2.2966e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to E:\\new yousef\\hand-gesture-recognition\\models\\asl_gesture_recognition_model.h5\n",
      "Validation Accuracy: 1.0000\n",
      "Validation Precision: 1.0000\n",
      "Validation Recall: 1.0000\n",
      "Validation F1-Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# Set paths\n",
    "data_dir = r\"E:\\new yousef\\hand-gesture-recognition\\data\\processed\"\n",
    "model_save_path = r\"E:\\new yousef\\hand-gesture-recognition\\models\\asl_gesture_recognition_model.h5\"\n",
    "img_height = 64\n",
    "img_width = 64\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "num_classes = 5\n",
    "\n",
    "# Define class names\n",
    "class_names = ['hello', 'yes', 'no', 'i love you', 'thank you']\n",
    "class_to_idx = {name: idx for idx, name in enumerate(class_names)}\n",
    "\n",
    "# Custom generator to load images in batches\n",
    "def data_generator(data_dir, batch_size, img_height, img_width, class_to_idx, shuffle=True):\n",
    "    # Filter valid filenames upfront\n",
    "    filenames = []\n",
    "    for f in os.listdir(data_dir):\n",
    "        if not f.endswith('.png'):\n",
    "            continue\n",
    "        sign_name = re.match(r'([^_]+)_processed_', f)\n",
    "        if sign_name and sign_name.group(1) in class_names:\n",
    "            filenames.append(f)\n",
    "        else:\n",
    "            print(f\"Skipping invalid filename: {f}\")\n",
    "    \n",
    "    if not filenames:\n",
    "        raise ValueError(f\"No valid images found in {data_dir} with expected naming pattern '<sign_name>_processed_...'\")\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(filenames)\n",
    "    \n",
    "    i = 0\n",
    "    while True:\n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "        attempts = 0\n",
    "        max_attempts = len(filenames)  # Prevent infinite loop\n",
    "        \n",
    "        while len(batch_images) < batch_size and attempts < max_attempts:\n",
    "            if i >= len(filenames):\n",
    "                i = 0\n",
    "                if shuffle:\n",
    "                    np.random.shuffle(filenames)\n",
    "            \n",
    "            filename = filenames[i]\n",
    "            img_path = os.path.join(data_dir, filename)\n",
    "            try:\n",
    "                img = load_img(img_path, target_size=(img_height, img_width))\n",
    "                img_array = img_to_array(img) / 255.0\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to load image '{img_path}'. Skipping. Reason: {e}\")\n",
    "                i += 1\n",
    "                attempts += 1\n",
    "                continue\n",
    "            \n",
    "            label_idx = class_to_idx[re.match(r'([^_]+)_processed_', filename).group(1)]\n",
    "            batch_images.append(img_array)\n",
    "            batch_labels.append(label_idx)\n",
    "            i += 1\n",
    "            attempts += 1\n",
    "        \n",
    "        if not batch_images:\n",
    "            print(\"No valid images could be loaded for this batch. Stopping generator.\")\n",
    "            raise StopIteration\n",
    "        \n",
    "        batch_images = np.array(batch_images)\n",
    "        batch_labels = tf.keras.utils.to_categorical(batch_labels, num_classes=len(class_names))\n",
    "        yield batch_images, batch_labels\n",
    "\n",
    "# Count total images for steps per epoch\n",
    "valid_filenames = [f for f in os.listdir(data_dir) if f.endswith('.png') and re.match(r'([^_]+)_processed_', f) and re.match(r'([^_]+)_processed_', f).group(1) in class_names]\n",
    "total_images = len(valid_filenames)\n",
    "if total_images == 0:\n",
    "    print(f\"No valid images found in {data_dir}\")\n",
    "    exit()\n",
    "\n",
    "# Define steps per epoch and validation steps\n",
    "train_split = 0.8\n",
    "steps_per_epoch = max(1, int(total_images * train_split) // batch_size)\n",
    "validation_steps = max(1, int(total_images * (1 - train_split)) // batch_size)\n",
    "\n",
    "# Create generators\n",
    "train_generator = data_generator(data_dir, batch_size, img_height, img_width, class_to_idx, shuffle=True)\n",
    "val_generator = data_generator(data_dir, batch_size, img_height, img_width, class_to_idx, shuffle=False)\n",
    "\n",
    "# Define the model using MobileNetV2\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "base_model.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model (initial training)\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=epochs // 2,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "fine_tune_history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=epochs // 2,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "try:\n",
    "    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "    model.save(model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "except PermissionError:\n",
    "    print(f\"Error: No permission to save model to '{model_save_path}'. Try running as administrator or checking folder permissions.\")\n",
    "    exit()\n",
    "except OSError as e:\n",
    "    print(f\"Error: Failed to save model to '{model_save_path}'. Reason: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, data_dir, batch_size, img_height, img_width, class_to_idx, class_names, num_samples=5000):\n",
    "    val_gen = data_generator(data_dir, batch_size, img_height, img_width, class_to_idx, shuffle=False)\n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    samples_processed = 0\n",
    "    for batch_images, batch_labels in val_gen:\n",
    "        batch_pred = model.predict(batch_images, verbose=0)\n",
    "        y_true.extend(np.argmax(batch_labels, axis=1))\n",
    "        y_pred.extend(np.argmax(batch_pred, axis=1))\n",
    "        samples_processed += len(batch_images)\n",
    "        if samples_processed >= num_samples:\n",
    "            break\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Validation Precision: {precision:.4f}\")\n",
    "    print(f\"Validation Recall: {recall:.4f}\")\n",
    "    print(f\"Validation F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "evaluate_model(model, data_dir, batch_size, img_height, img_width, class_to_idx, class_names)\n",
    "\n",
    "# Plot training & validation accuracy and loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "train_acc = history.history['accuracy'] + fine_tune_history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy'] + fine_tune_history.history['val_accuracy']\n",
    "plt.plot(train_acc)\n",
    "plt.plot(val_acc)\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "train_loss = history.history['loss'] + fine_tune_history.history['loss']\n",
    "val_loss = history.history['val_loss'] + fine_tune_history.history['val_loss']\n",
    "plt.plot(train_loss)\n",
    "plt.plot(val_loss)\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_plots.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
